{"cells":[{"cell_type":"markdown","id":"03e86dbb","metadata":{"id":"03e86dbb"},"source":["Les modèles (physiques ou corrélatifs) des pneumatiques jouent un rôle essentiel dans la mise au points de scénarios de conception ainsi que dans l'évaluation des performances des nouvelles gammes ou de gammes existantes. Ainsi, le modèle de rigidité de dérive vu dans le TP portant sur l'Optimisation Bayesienne, peut être exploité à travers des chaines de simulation pour juger de la qualité de pneumatiques en terme de critère de comportement, d'adhérence, d'endurance ou encore de temps au tour. C'est à cette performance que nous allons nous intéresser ici.\n","\n","Plus précisément, l'exercice consiste à mettre en place un environnement de simulation basé sur de l'apprentissage par renforcement qui a pour objectif de trouver les controles optimaux à appliquer à un véhicule pour que ce dernier puisse parcourir un circuit circulaire avec la vitesse la plus élevée possible."]},{"cell_type":"markdown","id":"2e5a940b","metadata":{"id":"2e5a940b"},"source":["## Mise en place de l'environnement"]},{"cell_type":"markdown","id":"1130e4ac","metadata":{"id":"1130e4ac"},"source":["En l'occurence, les états que l'on va considérer pour notre environnement sont:\n","- $x$: position du véhicule selon la direction $\\vec{X}$\n","- $y$: position du véhicule selon la direction $\\vec{Y}$\n","- $\\psi$: l'angle de lacet du véhicule\n","- $\\dot{x}$: vitesse du véhicule selon la direction $\\vec{X}$\n","- $\\dot{y}$: vitesse du véhicule selon la direction $\\vec{Y}$\n","- $\\dot{\\psi}$: vitesse de lacet du véhicule\n","\n","Les actions qui seront utilisées sont:\n","- $v$: la vitesse\n","- $\\alpha$: l'angle de braquage\n","\n","L'environnement que l'on va exploiter s'appuie sur le package Gym de la société OpenAI (https://gym.openai.com/). Un tel environnement s'appuie sur l'utilisation d'objets héritant de la classe *gym.Env* et comportant les méthodes suivantes:\n","- **__init__**: constructeur définissant les expaces d'actions (*action_space*) et d'observations (*observation_space*)\n","- **reset**: méthode permettant de réinitialiser les états\n","- **step**: fonction qui prend en entrée les valeurs des actions et renvoie les nouveaux états de l'environnement, le reward ainsi qu'un booléen indiquant s'il est nécessaire de réinitialiser les états \n","- **render**: méthode qui affiche l'état de l'environnement et différentes informations le concernant "]},{"cell_type":"code","source":["pip install gym"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PiozaJ_P2czB","executionInfo":{"status":"ok","timestamp":1673357040456,"user_tz":-60,"elapsed":3923,"user":{"displayName":"Ganesh JHUGROO","userId":"05526744378227438867"}},"outputId":"e7799d07-2083-4f3d-a268-d766c174b87c"},"id":"PiozaJ_P2czB","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (5.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n"]}]},{"cell_type":"code","execution_count":null,"id":"9c5d6cfc","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"9c5d6cfc","executionInfo":{"status":"error","timestamp":1673358870910,"user_tz":-60,"elapsed":1770,"user":{"displayName":"Ganesh JHUGROO","userId":"05526744378227438867"}},"outputId":"292a7d5a-1ced-4d53-aa35-78619fe43ef1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b5aba445d746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#/gym_gmmcar/envs/circle_env.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mchemin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym_gmmcar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcircle_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chemin'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import tensorflow as tf\n","#tf.enable_eager_execution()\n","tf.executing_eagerly()\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib notebook\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","chemin = '/content/drive/MyDrive/Michelin/Sujet 4/ApprentissageparRenforcement'\n","#/gym_gmmcar/envs/circle_env.py\n","\n","from chemin.gym_gmmcar.envs import circle_env"]},{"cell_type":"code","source":["print(tf.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mRCUtYHlqax1","executionInfo":{"status":"ok","timestamp":1673354533818,"user_tz":-60,"elapsed":29,"user":{"displayName":"Ganesh JHUGROO","userId":"05526744378227438867"}},"outputId":"5d6a4e1e-3e24-4b4d-ebcc-d58a6b47fc18"},"id":"mRCUtYHlqax1","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.2\n"]}]},{"cell_type":"markdown","id":"7c2984f0","metadata":{"id":"7c2984f0"},"source":["Notre objectif étant de rester sur le cirucuit tout en allant le plus vite possible, quel(-s) reward(-s) peut-on envisager? Implémenter l'un d'entre eux en complétant la méthode *get_reward* de la classe *OttEnv* ci-dessous:"]},{"cell_type":"code","execution_count":null,"id":"5589fb6e","metadata":{"id":"5589fb6e","colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"status":"error","timestamp":1673354533821,"user_tz":-60,"elapsed":26,"user":{"displayName":"Ganesh JHUGROO","userId":"05526744378227438867"}},"outputId":"1b527e0c-4ccf-4a45-adf1-fa337f0e7af3"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-83c3b9bbb9df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mOttEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCircleEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mEnvironnement\u001b[0m \u001b[0mde\u001b[0m \u001b[0msimulation\u001b[0m \u001b[0mpour\u001b[0m \u001b[0mune\u001b[0m \u001b[0mvoiture\u001b[0m \u001b[0mde\u001b[0m \u001b[0mcourse\u001b[0m \u001b[0msuivant\u001b[0m \u001b[0mune\u001b[0m \u001b[0mtrajectoire\u001b[0m \u001b[0mcirculaire\u001b[0m \u001b[0maussi\u001b[0m \u001b[0mvite\u001b[0m \u001b[0mque\u001b[0m \u001b[0mpossible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CircleEnv' is not defined"]}],"source":["class OttEnv(CircleEnv):\n","    \"\"\"\n","    Environnement de simulation pour une voiture de course suivant une trajectoire circulaire aussi vite que possible\n","    \"\"\"\n","\n","    def __init__(\n","            self,\n","            target_velocity=1.0,\n","            radius=1.0,\n","            dt=0.035,\n","            model_type='BrushTireModel',\n","            robot_type='RCCar',\n","            mu_s=1.37,\n","            mu_k=1.96,\n","            eps=0.05\n","    ):\n","\n","        super().__init__(\n","            target_velocity=target_velocity,\n","            radius=radius,\n","            dt=dt,\n","            model_type=model_type,\n","            robot_type=robot_type,\n","            mu_s=mu_s,\n","            mu_k=mu_k\n","        )\n","\n","        self.eps = eps\n","\n","\n","    def get_reward(self, state, action):\n","        \"\"\"\n","        Définition de la fonction de Reward\n","        \"\"\"\n","        r = self.radius\n","        x, y, _, x_dot, y_dot, _ = state\n","        vitesse = np.sqrt(x_dot**2 + y_dot**2)\n","        distance = np.sqrt(x**2 + y**2) - r\n","\n","        # Reward à définir\n","        # reward = ...\n","        \n","        info = {}\n","        info['dist'] = distance\n","        info['vel'] = vitesse\n","        return reward, info"]},{"cell_type":"markdown","id":"2fed2b78","metadata":{"id":"2fed2b78"},"source":["### Test de l'Environnement"]},{"cell_type":"markdown","id":"6b3c92cd","metadata":{"id":"6b3c92cd"},"source":["Tester l'environnement en considérant un épisode de 100 pas de temps et des actions aléatoires et/ou fixes. Pour ce faire, compléter le script ci-dessous en définissant les actions à appliquer à chaque pas."]},{"cell_type":"code","execution_count":null,"id":"a18d5d0a","metadata":{"id":"a18d5d0a"},"outputs":[],"source":["env = OttEnv()\n","obs = env.reset()\n","env.render()\n","\n","episode = 1\n","for step in range(100):\n","#     action = ...\n","    new_state, reward, done, info = env.step(action)\n","    env.render()"]},{"cell_type":"markdown","id":"7efff82c","metadata":{"id":"7efff82c"},"source":["Etant donné les caractéristiques du problème considéré, quel type de méthode devrait-on appliquer?"]},{"cell_type":"markdown","id":"cefea573","metadata":{"id":"cefea573"},"source":["## Implémentation de l'Algorithme Deep Deterministic Policy Gradient (DDPG)"]},{"cell_type":"markdown","id":"f16ad190","metadata":{"id":"f16ad190"},"source":["Pour tenter de trouver les commandes optimales à appliquer, nous allons ici utiliser une approche DDPG. Pour ce faire, la première étape à réaliser est d'implémenter cette méthode en s'appuyant sur le pseudo-code suivant vu en cours:\n","![DDPG.png](DDPG.png \"Algorithme DDPG\")"]},{"cell_type":"markdown","id":"50d06dda","metadata":{"id":"50d06dda"},"source":["### Création de l'Acteur"]},{"cell_type":"markdown","id":"c5d82702","metadata":{"id":"c5d82702"},"source":["Pour rappel, l'acteur a pour objectif d'estimer un politique $\\mu(s_{t})$. Dans un premier temps, créer un acteur à partir d'une fonction ou d'une classe en définissant un modèle neuronal tensorflow ayant l'architecture suivante:\n","- une première couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n","- une seconde couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n","- une couche de sortie dense comportant un nombre de neurones égal au nombre d'actions et une fonction d'activation de type tanh\n","\n","<ins>**Remarque:**</ins> Les sorties étant bornées entre -1 et 1, ne pas oublier de dénormaliser pour générer des valeurs d'actions conformes à l'espace des actions.\n","<ins>**Conseil:**</ins> Pour pouvoir tester différentes architectures par la suite, paramétrer les couches à l'aide d'une variable indiquant le nombre de neurones. "]},{"cell_type":"code","execution_count":null,"id":"bc5bece5","metadata":{"id":"bc5bece5"},"outputs":[],"source":["# Définition de l'acteur \n","\n"]},{"cell_type":"markdown","id":"01f7508c","metadata":{"id":"01f7508c"},"source":["### Création du Critique"]},{"cell_type":"markdown","id":"b3482374","metadata":{"id":"b3482374"},"source":["Pour rappel, le critique a pour objectif d'estimer la valeur $Q(s_{t},a_{t})$Dans un premier temps, créer un critique à partir d'une fonction ou d'une classe en définissant un modèle neuronal tensorflow de la manière suivante:\n","- Créer un réseau prenant en entrée les états avec:\n","  - une première couche cachée dense comportant 16 neurones et une fonction d'activation de type RELU\n","  - une seconde couche cachée dense comportant 32 neurones et une fonction d'activation de type RELU\n","- Créer un réseau prenant en entrée les actions avec une couche cachée dense comportant 32 neurones et une fonction d'activation de type RELU\n","- Concaténer les sorties des 2 réseaux précédents via la méthode \"*Concatenate*\"\n","- Créer un réseau prenant les entrées la concaténation des tenseurs précédents avec:\n","  - une première couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n","  - une seconde couche cachée dense comportant 256 neurones et une fonction d'activation de type RELU\n","  - une couche de sortie dense comportant 1 neurone sans fonction d'activation \n","\n","<ins>**Conseil:**</ins> Pour pouvoir tester différentes architectures par la suite, paramétrer les couches à l'aide d'une variable indiquant le nombre de neurones. "]},{"cell_type":"code","execution_count":null,"id":"f56d0670","metadata":{"id":"f56d0670"},"outputs":[],"source":["# Définition du critique\n","\n","\n"]},{"cell_type":"markdown","id":"06becb5e","metadata":{"id":"06becb5e"},"source":["### Création du Générateur de Bruit"]},{"cell_type":"markdown","id":"dad7ce24","metadata":{"id":"dad7ce24"},"source":["Comme précisé en cours, l'approche DDPG génère les actions de manière déterministe, ce qui engendre mécaniquement une démarche purement basée sur de l'exploitation. Pour éviter d'être coincé dans un optimum local, il est nécessaire d'appliquer une stratégie d'exploration. En l'occurrence, cette exploration est gérée via l'ajout d'un bruit à l'action générée par l'acteur.\n","Ce bruit est généré via un processus stochastique de type ***Ornstein-Uhlenbeck*** défini par l'équation différentielle stochastique:\n","\n","$dx_{t}=\\theta(\\nu-x_{t})dt+\\sigma\\sqrt{d_{t}}u$ avec $u\\sim \\mathcal{N}(0,1)$\n","\n","Créer une fonction ou classe permettant de générer ce bruit avec $\\theta=0.15$ et $d_{t}=1e-2$."]},{"cell_type":"code","execution_count":null,"id":"46121e8c","metadata":{"id":"46121e8c"},"outputs":[],"source":["# Définition du génrateur de bruit\n","\n","\n"]},{"cell_type":"markdown","id":"5dcd2454","metadata":{"id":"5dcd2454"},"source":["### Gestion de l'Experience Replay"]},{"cell_type":"markdown","id":"4c58d6db","metadata":{"id":"4c58d6db"},"source":["Afin de ne pas oublier les expériences passées et réduire les corrélations entre expériences, un tirage aléatoire de $N$ tuples (état présent, action, reward, état suivant) stockés dans un buffer de taille $B$.\n","Créer une fonction ou classe permettant de:\n","- Initialiser un buffer de taille $B$ à 0\n","- Sauvegarder à chaque pas de temps un 4-uplet (état présent, action, reward, état suivant)\n","- Tirer aléatoirement $N$ tuples (état présent, action, reward, état suivant)"]},{"cell_type":"code","execution_count":null,"id":"a46a7b29","metadata":{"id":"a46a7b29"},"outputs":[],"source":["# Définition du buffer\n","\n"]},{"cell_type":"markdown","id":"18db96a2","metadata":{"id":"18db96a2"},"source":["### Mise à jour des réseaux cibles"]},{"cell_type":"markdown","id":"5ed4148c","metadata":{"id":"5ed4148c"},"source":["Comme présenté en cours, la gestion des cibles mouvantes se fait via la mise en place de réseaux cibles. En l'occurrence, deux réseaux cibles sont utilisés: l'un pour l'acteur et l'autre pour le critique.\n","Créer une fonction ou classe qui mette à jour les poids des réseaux cibles."]},{"cell_type":"code","execution_count":null,"id":"dc74a9f4","metadata":{"id":"dc74a9f4"},"outputs":[],"source":["# Mise à jour des réseaux cibles\n","\n"]},{"cell_type":"markdown","id":"abaa8330","metadata":{"id":"abaa8330"},"source":["### Apprentissage"]},{"cell_type":"markdown","id":"a8cb8611","metadata":{"id":"a8cb8611"},"source":["Utiliser l'ensembles des fonctions/classes précédemment construites pour implémenter l'apprentissage présenté par le pseudo-code apparaissant plus haut avec les paramètres suivants:\n","- learning rate de l'acteur:0.002\n","- learning rate du critique: 0.001\n","- paramètre du générateur de bruit $\\sigma$: 0.2\n","- paramètre du générateur de bruit $\\nu$: 0\n","- nombre totale d'épisode $M$: 100\n","- facteur d'escompte $\\gamma$: 0.99\n","- paramètre mise à jour des réseaux cible $\\tau$: 0.005\n","- taille du buffer $B$: 1000\n","- taille $N$ des batchs: 100\n","\n","Pour pouvoir mener un diagnosqtique de l'apprentissage, stocker les rewards cumulés à la fin de chaque épisode dans une liste."]},{"cell_type":"code","execution_count":null,"id":"23c6dbea","metadata":{"id":"23c6dbea"},"outputs":[],"source":["# Algorithme DDPG\n","\n"]},{"cell_type":"markdown","id":"fdbc6fbb","metadata":{"id":"fdbc6fbb"},"source":["### Diagnostique"]},{"cell_type":"markdown","id":"bc367ae6","metadata":{"id":"bc367ae6"},"source":["Afficher l'évolution de la moyenne des rewards cumulés calculée tous les 20 épisodes."]},{"cell_type":"code","execution_count":null,"id":"2170fc50","metadata":{"id":"2170fc50"},"outputs":[],"source":["# Affichage de la moyenne des rewards cumulés$\n","\n"]},{"cell_type":"markdown","id":"83bad3e6","metadata":{"id":"83bad3e6"},"source":["### Réglage des paramètres d'apprentissage"]},{"cell_type":"markdown","id":"57f40573","metadata":{"id":"57f40573"},"source":["Essayer différents paramètres utilisés lors de l'apprentissage ainsi que différentes architecture de réseaux pour  nombres de points d'évaluation et les fonctions d'acquisition PI et LCB pour tenter d'améliorer le paramétrage de l'algorithme. "]},{"cell_type":"code","execution_count":null,"id":"8102ff6f","metadata":{"id":"8102ff6f"},"outputs":[],"source":["# Tests et évaluation avec différentes configurations\n","\n"]}],"metadata":{"kernelspec":{"display_name":"gmm_common_env","language":"python","name":"gmm_common_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}